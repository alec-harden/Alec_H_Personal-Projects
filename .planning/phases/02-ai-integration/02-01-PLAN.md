---
phase: 02-ai-integration
plan: 01
type: execute
wave: 1
depends_on: ["01-02"]
files_modified:
  - package.json
  - src/lib/server/ai.ts
  - src/routes/api/chat/+server.ts
  - .env.example
autonomous: true
must_haves:
  truths:
    - "POST /api/chat returns streaming response"
    - "AI_PROVIDER=anthropic uses Claude model"
    - "AI_PROVIDER=openai uses GPT model"
  artifacts:
    - path: "src/lib/server/ai.ts"
      provides: "AI provider factory"
      exports: ["getModel"]
    - path: "src/routes/api/chat/+server.ts"
      provides: "Streaming chat endpoint"
      exports: ["POST"]
  key_links:
    - from: "src/routes/api/chat/+server.ts"
      to: "src/lib/server/ai.ts"
      via: "import { getModel }"
      pattern: "import.*getModel.*from.*ai"
---

<objective>
Set up Vercel AI SDK with configurable providers and streaming chat endpoint.

Purpose: Establish AI conversation infrastructure that future phases will build upon for BOM generation.
Output: Working `/api/chat` endpoint that streams AI responses, with provider switchable via environment variable.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-foundation/01-01-SUMMARY.md
@.planning/phases/01-foundation/01-02-SUMMARY.md

Relevant source files:
@src/lib/server/db.ts - Pattern for env-based server configuration
</context>

<tasks>

<task type="auto">
  <name>Task 1: Install AI SDK packages and create provider factory</name>
  <files>package.json, src/lib/server/ai.ts, .env.example</files>
  <action>
1. Install Vercel AI SDK packages:
   ```bash
   npm install ai @ai-sdk/svelte @ai-sdk/anthropic @ai-sdk/openai zod
   ```

2. Update `.env.example` to document AI configuration (append to existing content):
   ```
   # AI Provider Configuration
   # Options: anthropic | openai
   AI_PROVIDER=anthropic

   # Provider API Keys (only need the one matching AI_PROVIDER)
   ANTHROPIC_API_KEY=your-anthropic-key
   OPENAI_API_KEY=your-openai-key
   ```

3. Create `src/lib/server/ai.ts` with provider factory pattern (follow db.ts pattern for env imports):
   ```typescript
   import { anthropic } from '@ai-sdk/anthropic';
   import { openai } from '@ai-sdk/openai';
   import { AI_PROVIDER } from '$env/static/private';

   export type AIProvider = 'anthropic' | 'openai';

   const provider = (AI_PROVIDER || 'anthropic') as AIProvider;

   export function getModel() {
     switch (provider) {
       case 'openai':
         return openai('gpt-4o');
       case 'anthropic':
       default:
         return anthropic('claude-sonnet-4-20250514');
     }
   }

   export { provider as currentProvider };
   ```

IMPORTANT: The user must add their API keys to .env manually - do NOT create or modify .env file with actual keys.
  </action>
  <verify>npm run build succeeds without TypeScript errors</verify>
  <done>AI SDK packages installed, provider factory exports getModel() function</done>
</task>

<task type="auto">
  <name>Task 2: Create streaming chat endpoint</name>
  <files>src/routes/api/chat/+server.ts</files>
  <action>
1. Create directory structure: `src/routes/api/chat/`

2. Create `src/routes/api/chat/+server.ts`:
   ```typescript
   import { streamText } from 'ai';
   import { getModel } from '$lib/server/ai';
   import type { RequestHandler } from './$types';

   export const POST: RequestHandler = async ({ request }) => {
     const { messages } = await request.json();

     const result = streamText({
       model: getModel(),
       messages,
       system: 'You are a helpful woodworking assistant that helps users create bills of materials for their projects. Be concise and practical. When discussing materials, be specific about dimensions, quantities, and types.',
     });

     return result.toUIMessageStreamResponse();
   };
   ```

This endpoint:
- Accepts POST with `{ messages: [...] }` body
- Uses the configured AI provider via getModel()
- Includes woodworking-focused system prompt
- Returns streaming response compatible with AI SDK's Chat class
  </action>
  <verify>
1. npm run build succeeds
2. npm run dev starts without errors
3. Test endpoint with curl (will fail without API key but should not throw TypeScript errors)
  </verify>
  <done>Chat endpoint exists at /api/chat, accepts messages, streams responses</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `npm run build` succeeds without errors
- [ ] `npm run check` passes TypeScript validation
- [ ] `src/lib/server/ai.ts` exports `getModel` function
- [ ] `src/routes/api/chat/+server.ts` exports POST handler
- [ ] `.env.example` documents AI_PROVIDER and API key variables
</verification>

<success_criteria>
- All tasks completed
- All verification checks pass
- No TypeScript errors
- AI provider factory pattern established
- Chat endpoint ready for UI integration
</success_criteria>

<output>
After completion, create `.planning/phases/02-ai-integration/02-01-SUMMARY.md`
</output>
